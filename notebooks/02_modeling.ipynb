{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6216002",
   "metadata": {},
   "source": [
    "cell 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "699e2e38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Provider</th>\n",
       "      <th>inp_claims</th>\n",
       "      <th>inp_total_reimbursed</th>\n",
       "      <th>inp_mean_reimbursed</th>\n",
       "      <th>inp_max_reimbursed</th>\n",
       "      <th>inp_mean_deductible</th>\n",
       "      <th>inp_mean_los</th>\n",
       "      <th>inp_unique_bene</th>\n",
       "      <th>outp_claims</th>\n",
       "      <th>outp_total_reimbursed</th>\n",
       "      <th>...</th>\n",
       "      <th>mean_ChronicCond_Cancer</th>\n",
       "      <th>mean_ChronicCond_ObstrPulmonary</th>\n",
       "      <th>mean_ChronicCond_Depression</th>\n",
       "      <th>mean_ChronicCond_Diabetes</th>\n",
       "      <th>mean_ChronicCond_IschemicHeart</th>\n",
       "      <th>mean_ChronicCond_Osteoporasis</th>\n",
       "      <th>mean_ChronicCond_rheumatoidarthritis</th>\n",
       "      <th>mean_ChronicCond_stroke</th>\n",
       "      <th>PotentialFraud</th>\n",
       "      <th>FraudLabel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PRV51001</td>\n",
       "      <td>5.0</td>\n",
       "      <td>97000.0</td>\n",
       "      <td>19400.000000</td>\n",
       "      <td>42000.0</td>\n",
       "      <td>1068.0</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>7640.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PRV51003</td>\n",
       "      <td>62.0</td>\n",
       "      <td>573000.0</td>\n",
       "      <td>9241.935484</td>\n",
       "      <td>57000.0</td>\n",
       "      <td>1068.0</td>\n",
       "      <td>5.161290</td>\n",
       "      <td>53.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>32670.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.887097</td>\n",
       "      <td>1.629032</td>\n",
       "      <td>1.596774</td>\n",
       "      <td>1.209677</td>\n",
       "      <td>1.112903</td>\n",
       "      <td>1.790323</td>\n",
       "      <td>1.693548</td>\n",
       "      <td>1.887097</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PRV51004</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>149.0</td>\n",
       "      <td>52170.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PRV51005</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1165.0</td>\n",
       "      <td>280910.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PRV51007</td>\n",
       "      <td>3.0</td>\n",
       "      <td>19000.0</td>\n",
       "      <td>6333.333333</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>1068.0</td>\n",
       "      <td>5.333333</td>\n",
       "      <td>3.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>14710.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows \u00d7 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Provider  inp_claims  inp_total_reimbursed  inp_mean_reimbursed  \\\n",
       "0  PRV51001         5.0               97000.0         19400.000000   \n",
       "1  PRV51003        62.0              573000.0          9241.935484   \n",
       "2  PRV51004         NaN                   NaN                  NaN   \n",
       "3  PRV51005         NaN                   NaN                  NaN   \n",
       "4  PRV51007         3.0               19000.0          6333.333333   \n",
       "\n",
       "   inp_max_reimbursed  inp_mean_deductible  inp_mean_los  inp_unique_bene  \\\n",
       "0             42000.0               1068.0      5.000000              5.0   \n",
       "1             57000.0               1068.0      5.161290             53.0   \n",
       "2                 NaN                  NaN           NaN              NaN   \n",
       "3                 NaN                  NaN           NaN              NaN   \n",
       "4             10000.0               1068.0      5.333333              3.0   \n",
       "\n",
       "   outp_claims  outp_total_reimbursed  ...  mean_ChronicCond_Cancer  \\\n",
       "0         20.0                 7640.0  ...                 1.800000   \n",
       "1         70.0                32670.0  ...                 1.887097   \n",
       "2        149.0                52170.0  ...                      NaN   \n",
       "3       1165.0               280910.0  ...                      NaN   \n",
       "4         69.0                14710.0  ...                 2.000000   \n",
       "\n",
       "   mean_ChronicCond_ObstrPulmonary  mean_ChronicCond_Depression  \\\n",
       "0                         1.600000                     1.200000   \n",
       "1                         1.629032                     1.596774   \n",
       "2                              NaN                          NaN   \n",
       "3                              NaN                          NaN   \n",
       "4                         2.000000                     1.333333   \n",
       "\n",
       "   mean_ChronicCond_Diabetes  mean_ChronicCond_IschemicHeart  \\\n",
       "0                   1.200000                        1.200000   \n",
       "1                   1.209677                        1.112903   \n",
       "2                        NaN                             NaN   \n",
       "3                        NaN                             NaN   \n",
       "4                   1.000000                        1.000000   \n",
       "\n",
       "   mean_ChronicCond_Osteoporasis  mean_ChronicCond_rheumatoidarthritis  \\\n",
       "0                       2.000000                              1.400000   \n",
       "1                       1.790323                              1.693548   \n",
       "2                            NaN                                   NaN   \n",
       "3                            NaN                                   NaN   \n",
       "4                       2.000000                              1.666667   \n",
       "\n",
       "   mean_ChronicCond_stroke  PotentialFraud  FraudLabel  \n",
       "0                 1.600000              No           0  \n",
       "1                 1.887097             Yes           1  \n",
       "2                      NaN              No           0  \n",
       "3                      NaN             Yes           1  \n",
       "4                 1.333333              No           0  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Imports & load processed data\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    "    average_precision_score\n",
    ")\n",
    "\n",
    "DATA_DIR = \"../data\"\n",
    "PROCESSED_DIR = os.path.join(DATA_DIR, \"processed\")\n",
    "\n",
    "train_full = pd.read_csv(os.path.join(PROCESSED_DIR, \"train_provider_features.csv\"))\n",
    "test_full  = pd.read_csv(os.path.join(PROCESSED_DIR, \"test_provider_features.csv\"))\n",
    "\n",
    "train_full.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65625b8e",
   "metadata": {},
   "source": [
    "## Notebook 02 Goals\n",
    "- Train multiple models\n",
    "- Compare Logistic Regression, Random Forest, Gradient Boosting\n",
    "- Evaluate with PR-AUC and ROC-AUC\n",
    "- Select best model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f9a0a4",
   "metadata": {},
   "source": [
    "cell 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d9021f2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping all-NaN columns: set()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['../data/processed/best_model.pkl']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# 1) Choose feature columns (everything except ID + labels)\n",
    "feature_cols = [c for c in train_full.columns \n",
    "                if c not in [\"Provider\", \"PotentialFraud\", \"FraudLabel\"]]\n",
    "\n",
    "X_train_full = train_full[feature_cols]\n",
    "y = train_full[\"FraudLabel\"]\n",
    "\n",
    "X_test_full = test_full[feature_cols].copy()\n",
    "\n",
    "# 2) Drop columns that are all-NaN in TRAIN or TEST\n",
    "cols_all_nan_train = X_train_full.columns[X_train_full.isna().all()]\n",
    "cols_all_nan_test  = X_test_full.columns[X_test_full.isna().all()]\n",
    "\n",
    "drop_cols = set(cols_all_nan_train) | set(cols_all_nan_test)\n",
    "\n",
    "print(\"Dropping all-NaN columns:\", drop_cols)\n",
    "\n",
    "feature_cols = [c for c in feature_cols if c not in drop_cols]\n",
    "X_train_full = X_train_full[feature_cols]\n",
    "X_test_full  = X_test_full[feature_cols]\n",
    "\n",
    "# 3) Impute with medians from TRAIN ONLY\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "X_train_imputed = imputer.fit_transform(X_train_full)\n",
    "X_test_imputed  = imputer.transform(X_test_full)\n",
    "\n",
    "# 4) Scale\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_train_imputed)\n",
    "\n",
    "# 5) Train/validation split on SCALED data\n",
    "indices = np.arange(len(y))\n",
    "\n",
    "X_train, X_val, y_train, y_val, idx_train, idx_val = train_test_split(\n",
    "    X_scaled,\n",
    "    y,\n",
    "    indices,\n",
    "    test_size=0.2,\n",
    "    stratify=y,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Save these for Notebook 3\n",
    "np.save(os.path.join(PROCESSED_DIR, \"X_val.npy\"), X_val)\n",
    "np.save(os.path.join(PROCESSED_DIR, \"y_val.npy\"), y_val)\n",
    "np.save(os.path.join(PROCESSED_DIR, \"val_indices.npy\"), idx_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515616d2",
   "metadata": {},
   "source": [
    "cell 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7e809235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== logreg ===\n",
      "ROC-AUC: 0.9494353105035275\n",
      "PR-AUC : 0.7437851118802034\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.91      0.94       981\n",
      "           1       0.47      0.82      0.60       101\n",
      "\n",
      "    accuracy                           0.90      1082\n",
      "   macro avg       0.73      0.86      0.77      1082\n",
      "weighted avg       0.93      0.90      0.91      1082\n",
      "\n",
      "Confusion matrix:\n",
      " [[889  92]\n",
      " [ 18  83]]\n",
      "\n",
      "=== rf ===\n",
      "ROC-AUC: 0.95795359352449\n",
      "PR-AUC : 0.7497441240281424\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.98      0.97       981\n",
      "           1       0.73      0.53      0.62       101\n",
      "\n",
      "    accuracy                           0.94      1082\n",
      "   macro avg       0.84      0.76      0.79      1082\n",
      "weighted avg       0.93      0.94      0.93      1082\n",
      "\n",
      "Confusion matrix:\n",
      " [[961  20]\n",
      " [ 47  54]]\n",
      "\n",
      "=== gb ===\n",
      "ROC-AUC: 0.9642867956520422\n",
      "PR-AUC : 0.7789618517950189\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.98      0.97       981\n",
      "           1       0.75      0.60      0.67       101\n",
      "\n",
      "    accuracy                           0.94      1082\n",
      "   macro avg       0.86      0.79      0.82      1082\n",
      "weighted avg       0.94      0.94      0.94      1082\n",
      "\n",
      "Confusion matrix:\n",
      " [[961  20]\n",
      " [ 40  61]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('gb', np.float64(0.9642867956520422), np.float64(0.7789618517950189))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Model training and evaluation\n",
    "models = {\n",
    "    \"logreg\": LogisticRegression(max_iter=1000, class_weight=\"balanced\"),\n",
    "    \"rf\": RandomForestClassifier(\n",
    "        n_estimators=300,\n",
    "        random_state=42,\n",
    "        class_weight=\"balanced\"\n",
    "    ),\n",
    "    \"gb\": GradientBoostingClassifier(random_state=42),\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, clf in models.items():\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_val)\n",
    "    y_proba = clf.predict_proba(X_val)[:, 1]\n",
    "\n",
    "    roc = roc_auc_score(y_val, y_proba)\n",
    "    pr  = average_precision_score(y_val, y_proba)\n",
    "\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    print(\"ROC-AUC:\", roc)\n",
    "    print(\"PR-AUC :\", pr)\n",
    "    print(classification_report(y_val, y_pred))\n",
    "    print(\"Confusion matrix:\\n\", confusion_matrix(y_val, y_pred))\n",
    "\n",
    "    results[name] = {\"model\": clf, \"roc_auc\": roc, \"pr_auc\": pr}\n",
    "\n",
    "#Select best model and save\n",
    "best_name = max(results.keys(), key=lambda k: results[k][\"pr_auc\"])\n",
    "best_model = results[best_name][\"model\"]\n",
    "best_name, results[best_name][\"roc_auc\"], results[best_name][\"pr_auc\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f57beb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model baseline comparison table\n",
    "roc_scores = {k: v['roc_auc'] for k, v in results.items()}\n",
    "pr_scores = {k: v['pr_auc'] for k, v in results.items()}\n",
    "\n",
    "comparison = pd.DataFrame({\n",
    "    'Model': list(roc_scores.keys()),\n",
    "    'ROC-AUC': list(roc_scores.values()),\n",
    "    'PR-AUC': list(pr_scores.values())\n",
    "})\n",
    "comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f56034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC and Precision-Recall curves for the selected best model\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "best_proba = best_model.predict_proba(X_val)[:, 1]\n",
    "fpr, tpr, _ = roc_curve(y_val, best_proba)\n",
    "prec, rec, _ = precision_recall_curve(y_val, best_proba)\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.plot(fpr, tpr, label=f\"ROC (AUC = {results[best_name]['roc_auc']:.3f})\")\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Random')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve - Validation')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.plot(rec, prec, label=f\"PR (AUC = {results[best_name]['pr_auc']:.3f})\")\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve - Validation')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a8bf00",
   "metadata": {},
   "source": [
    "### Metric Justification\n",
    "- Fraud data are highly imbalanced, so precision and recall matter more than accuracy.\n",
    "- PR-AUC emphasizes performance on the positive (fraud) class, making it more informative here than ROC-AUC.\n",
    "- We therefore prioritize the model with the highest PR-AUC when selecting the winner.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9566b637",
   "metadata": {},
   "source": [
    "cell 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3da453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Retrain best model on ALL training data (using the same imputed & scaled features)\n",
    "\n",
    "# Recompute scaled FULL train (we already have X_train_imputed & medians & scaler)\n",
    "X_full_imputed = X_train_imputed  # all providers in train_full\n",
    "X_full_scaled  = scaler.fit_transform(X_full_imputed)  # fit scaler on full train\n",
    "\n",
    "best_model.fit(X_full_scaled, y)\n",
    "\n",
    "# 2) Prepare TEST set: impute with same medians, scale with same scaler\n",
    "X_test_imputed = X_test_imputed  # already filled with medians earlier\n",
    "X_test_scaled  = scaler.transform(X_test_imputed)\n",
    "\n",
    "import numpy as np\n",
    "print(\"Any NaN in X_test_scaled?\", np.isnan(X_test_scaled).any())\n",
    "\n",
    "# 3) Predict probabilities\n",
    "test_proba = best_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"Provider\": test_full[\"Provider\"],\n",
    "    \"FraudProbability\": test_proba\n",
    "})\n",
    "\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0812619",
   "metadata": {},
   "source": [
    "cell 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09aff43a",
   "metadata": {},
   "source": [
    "### Why Gradient Boosting Was Selected\n",
    "- Delivers the best PR-AUC among the tested models.\n",
    "- Captures nonlinear interactions across claim features.\n",
    "- Sequential boosting reduces residual errors from prior learners.\n",
    "- Outperforms Random Forest on this imbalanced fraud problem.\n",
    "- More flexible than Logistic Regression for complex provider behavior.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3bc822dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/processed/imputer.pkl']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Save model\n",
    "joblib.dump(best_model, \"../data/processed/best_model.pkl\")\n",
    "\n",
    "# Save validation data used for evaluation\n",
    "np.save(\"../data/processed/X_val.npy\", X_val)\n",
    "np.save(\"../data/processed/y_val.npy\", y_val)\n",
    "\n",
    "# Save scaler + imputer too if needed\n",
    "joblib.dump(scaler, \"../data/processed/scaler.pkl\")\n",
    "joblib.dump(imputer, \"../data/processed/imputer.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c47e7fc",
   "metadata": {},
   "source": [
    "### Export Artifacts for Notebook 03\n",
    "Saving the trained model, scaler, and imputer so Notebook 03 can load and evaluate them consistently.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9bfc99",
   "metadata": {},
   "source": [
    "cell 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "14d63d96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ../data/processed/provider_fraud_predictions.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/m.mohamedali/Library/Python/3.9/lib/python/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Rebuild feature column list (same as training)\n",
    "feature_cols = [\n",
    "    c for c in test_full.columns\n",
    "    if c not in [\"Provider\"]     # keep provider ID out of X\n",
    "]\n",
    "\n",
    "# Prepare X_test\n",
    "X_test = test_full[feature_cols].copy()\n",
    "\n",
    "# Impute missing values (same imputer used on train)\n",
    "X_test_imputed = imputer.transform(X_test)\n",
    "\n",
    "# Scale features (same scaler used on train)\n",
    "X_test_scaled = scaler.transform(X_test_imputed)\n",
    "\n",
    "# Predict fraud probability\n",
    "test_proba = best_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Create final dataframe\n",
    "submission = pd.DataFrame({\n",
    "    \"Provider\": test_full[\"Provider\"],\n",
    "    \"FraudProbability\": test_proba\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "output_path = os.path.join(PROCESSED_DIR, \"provider_fraud_predictions.csv\")\n",
    "submission.to_csv(output_path, index=False)\n",
    "\n",
    "print(\"Saved:\", output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90278c54",
   "metadata": {},
   "source": [
    "### Notebook 02 Summary\n",
    "- Purpose: train and compare multiple models for provider fraud detection.\n",
    "- Comparison table highlights ROC-AUC and PR-AUC across Logistic Regression, Random Forest, and Gradient Boosting.\n",
    "- Metric choice: PR-AUC prioritized due to class imbalance.\n",
    "- Gradient Boosting chosen based on superior PR-AUC and ability to model nonlinear patterns.\n",
    "- Outputs exported (model + preprocessing) for downstream evaluation in Notebook 03.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}